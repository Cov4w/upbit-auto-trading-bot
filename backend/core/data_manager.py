"""
Data & Model Manager
=====================
í•™ìŠµ ë°ì´í„° ì €ì¥ì†Œ(TradeMemory)ì™€ ëª¨ë¸ ê´€ë¦¬(ModelLearner) í´ë˜ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
ì´ëŠ” 'Self-Evolving Trading System'ì˜ í•µì‹¬ ë‘ë‡Œ ì—­í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

Core Concepts:
- TradeMemory: ë§¤ë§¤ ê²°ê³¼ë¥¼ ì˜êµ¬ ì €ì¥ (SQLite)
- ModelLearner: XGBoost ëª¨ë¸ì˜ í•™ìŠµ/ì¬í•™ìŠµ/ì˜ˆì¸¡ ê´€ë¦¬
- Feature Engineering: ê¸°ìˆ ì  ì§€í‘œë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ íŠ¹ì§• ì¶”ì¶œ
"""

import sqlite3
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from pathlib import Path
import joblib
from typing import Dict, Tuple, Optional
import logging
import os

# Machine Learning
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report

# Technical Indicators
from ta.trend import SMAIndicator, EMAIndicator, MACD
from ta.momentum import RSIIndicator, StochasticOscillator
from ta.volatility import BollingerBands, AverageTrueRange

# Setup Logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì°¾ê¸° (backend/core/data_manager.py -> ../../)
def get_project_root() -> Path:
    """í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ë°˜í™˜ (backend/coreì—ì„œ 2ë‹¨ê³„ ìœ„)"""
    current_file = Path(__file__).resolve()
    # backend/core/data_manager.py -> backend/core -> backend -> project_root
    project_root = current_file.parent.parent.parent
    return project_root


PROJECT_ROOT = get_project_root()


class TradeMemory:
    """
    ë§¤ë§¤ ê¸°ë¡ ë° í•™ìŠµ ë°ì´í„° ì˜êµ¬ ì €ì¥ì†Œ
    
    ë§¤ë§¤ê°€ ì™„ë£Œë  ë•Œë§ˆë‹¤ ì§„ì… ì‹œì ì˜ íŠ¹ì§•(Features)ê³¼ ê²°ê³¼(Profit/Loss)ë¥¼
    SQLite DBì— ì €ì¥í•˜ì—¬ ëª¨ë¸ì´ ì‹¤ì „ ë°ì´í„°ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, db_path: str = None):
        # ê¸°ë³¸ ê²½ë¡œëŠ” í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ data/trade_memory.db
        if db_path is None:
            db_path = str(PROJECT_ROOT / "data" / "trade_memory.db")
        self.db_path = db_path
        # ë””ë ‰í† ë¦¬ ìƒì„±
        Path(db_path).parent.mkdir(parents=True, exist_ok=True)
        self._init_database()
        logger.info(f"âœ… TradeMemory initialized at {db_path}")
    
    def _init_database(self):
        """ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸” ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            # ë§¤ë§¤ ê¸°ë¡ í…Œì´ë¸”
            conn.execute("""
                CREATE TABLE IF NOT EXISTS trades (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    ticker TEXT NOT NULL,
                    entry_price REAL NOT NULL,
                    exit_price REAL,
                    profit_rate REAL,
                    is_profitable INTEGER,  -- 1: ìˆ˜ìµ, 0: ì†ì‹¤
                    profit_class INTEGER,   -- ğŸ†• 0: í°ì†ì‹¤, 1: ì†Œí­, 2: ì¢‹ì€ìˆ˜ìµ
                    
                    -- Technical Features (ì§„ì… ì‹œì )
                    rsi REAL,
                    macd REAL,
                    macd_signal REAL,
                    bb_position REAL,  -- Bollinger Band ìƒëŒ€ ìœ„ì¹˜
                    volume_ratio REAL,
                    price_change_5m REAL,
                    price_change_15m REAL,
                    ema_9 REAL,
                    ema_21 REAL,
                    atr REAL,
                    
                    -- ğŸ†• Time Features (ì‹œê°„ íŠ¹ì§•)
                    hour_of_day INTEGER,    -- 0-23
                    day_of_week INTEGER,    -- 0-6 (ì›”-ì¼)
                    
                    -- ğŸ†• Momentum Features (ëª¨ë©˜í…€ íŠ¹ì§•)
                    rsi_change REAL,        -- RSI ë³€í™”ëŸ‰ (5ë¶„)
                    volume_trend REAL,      -- ê±°ë˜ëŸ‰ ì¶”ì„¸
                    
                    -- ğŸ†• Sequence Features (ì‹œê³„ì—´ íŠ¹ì§•)
                    rsi_prev_5m REAL,       -- 5ë¶„ ì „ RSI
                    bb_position_prev_5m REAL,  -- 5ë¶„ ì „ BB ìœ„ì¹˜
                    
                    -- Model Prediction
                    model_confidence REAL,
                    
                    -- Status
                    status TEXT DEFAULT 'closed'  -- open, closed
                )
            """)
            
            # ëª¨ë¸ ì„±ëŠ¥ ì¶”ì  í…Œì´ë¸”
            conn.execute("""
                CREATE TABLE IF NOT EXISTS model_performance (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    total_trades INTEGER,
                    win_rate REAL,
                    accuracy REAL,
                    avg_profit REAL,
                    model_version TEXT
                )
            """)
            conn.commit()
    
    def save_trade_entry(self, ticker: str, entry_price: float, 
                        features: Dict, model_confidence: float) -> int:
        """
        ë§¤ìˆ˜ ì§„ì… ì‹œì  ë°ì´í„° ì €ì¥
        
        Args:
            ticker: ê±°ë˜ í‹°ì»¤
            entry_price: ì§„ì… ê°€ê²©
            features: ê¸°ìˆ ì  ì§€í‘œ íŠ¹ì§•ë“¤
            model_confidence: ëª¨ë¸ í™•ì‹ ë„
        
        Returns:
            trade_id: ì €ì¥ëœ ê±°ë˜ ID
        """
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                INSERT INTO trades (
                    timestamp, ticker, entry_price, model_confidence,
                    rsi, macd, macd_signal, bb_position, volume_ratio,
                    price_change_5m, price_change_15m, ema_9, ema_21, atr,
                    hour_of_day, day_of_week, rsi_change, volume_trend,
                    rsi_prev_5m, bb_position_prev_5m,
                    status
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, 'open')
            """, (
                datetime.now().isoformat(),
                ticker,
                entry_price,
                model_confidence,
                features.get('rsi', 0),
                features.get('macd', 0),
                features.get('macd_signal', 0),
                features.get('bb_position', 0),
                features.get('volume_ratio', 0),
                features.get('price_change_5m', 0),
                features.get('price_change_15m', 0),
                features.get('ema_9', 0),
                features.get('ema_21', 0),
                features.get('atr', 0),
                features.get('hour_of_day', 0),
                features.get('day_of_week', 0),
                features.get('rsi_change', 0),
                features.get('volume_trend', 0),
                features.get('rsi_prev_5m', 0),
                features.get('bb_position_prev_5m', 0)
            ))
            conn.commit()
            trade_id = cursor.lastrowid
            logger.info(f"ğŸ’¾ Trade Entry Saved: ID={trade_id}, Price={entry_price:,.0f}")
            return trade_id
    
    def update_trade_exit(self, trade_id: int, exit_price: float):
        """
        ë§¤ë„ ì™„ë£Œ ì‹œì  ë°ì´í„° ì—…ë°ì´íŠ¸ ë° ê²°ê³¼ ê¸°ë¡
        
        ì´ í•¨ìˆ˜ í˜¸ì¶œ í›„ ëª¨ë¸ ì¬í•™ìŠµì´ íŠ¸ë¦¬ê±°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        """
        try:
            with sqlite3.connect(self.db_path) as conn:
                # ì§„ì… ê°€ê²© ì¡°íšŒ
                result = conn.execute(
                    "SELECT entry_price FROM trades WHERE id = ?", 
                    (trade_id,)
                ).fetchone()
                
                # ğŸ›¡ï¸ Safety: DBì— í•´ë‹¹ ê±°ë˜ ê¸°ë¡ì´ ì—†ì„ ê²½ìš° (ìˆ˜ë™ ì§€ê°‘ ì¶”ê°€ ë“±)
                if not result:
                    logger.warning(f"âš ï¸ Trade ID={trade_id} not found in DB. Skipping update.")
                    return
                
                entry_price = result[0]
                
                # ìˆ˜ìµë¥  ê³„ì‚°
                profit_rate = (exit_price - entry_price) / entry_price
                
                # ğŸ”¥ ìˆ˜ìˆ˜ë£Œ(ì•½ 0.1%) ê³ ë ¤í•˜ì—¬ ì‹¤ì§ˆ ìˆ˜ìµì¼ ë•Œë§Œ ìŠ¹ë¦¬ë¡œ ì¸ì •
                # ì—…ë¹„íŠ¸: 0.05% + 0.05% = 0.1%
                is_profitable = 1 if profit_rate > 0.001 else 0
                
                # ğŸ†• profit_class: 3ë‹¨ê³„ ë¶„ë¥˜ (ìˆ˜ìµë¥  í¬ê¸° ë°˜ì˜)
                # 0: í° ì†ì‹¤ (< -0.5%)
                # 1: ì†Œí­/ë³¸ì „ (-0.5% ~ +0.5%)
                # 2: ì¢‹ì€ ìˆ˜ìµ (> +0.5%)
                if profit_rate < -0.005:
                    profit_class = 0  # Big loss
                elif profit_rate > 0.005:
                    profit_class = 2  # Good profit
                else:
                    profit_class = 1  # Neutral
                
                # ì—…ë°ì´íŠ¸
                conn.execute("""
                    UPDATE trades 
                    SET exit_price = ?,
                        profit_rate = ?,
                        is_profitable = ?,
                        profit_class = ?,
                        status = 'closed'
                    WHERE id = ?
                """, (exit_price, profit_rate, is_profitable, profit_class, trade_id))
                conn.commit()
                
                class_emoji = ["ğŸ”´", "âšª", "ğŸŸ¢"][profit_class]
                logger.info(
                    f"{class_emoji} Trade Closed: ID={trade_id}, "
                    f"Profit={profit_rate*100:.2f}% (Class={profit_class})"
                )
        except Exception as e:
            logger.error(f"âŒ Failed to update trade exit: {e}")
    
    def get_learning_data(self, min_samples: int = 30, limit: int = 500) -> Optional[Tuple[pd.DataFrame, pd.Series, np.ndarray]]:
        """
        ëª¨ë¸ í•™ìŠµìš© ë°ì´í„° ë°˜í™˜ (ì‹œê°„ ê°€ì¤‘ì¹˜ í¬í•¨)
        ğŸ†• Time-Weighted Learning: ìµœê·¼ ë°ì´í„°ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ ë¶€ì—¬

        Args:
            min_samples: ìµœì†Œ ë°ì´í„° ìˆ˜
            limit: ìµœëŒ€ ë°ì´í„° ìˆ˜ (125 â†’ 500ìœ¼ë¡œ ì¦ê°€, ì˜¤ë˜ëœ ë°ì´í„° ì‚­ì œí•˜ì§€ ì•ŠìŒ)

        Returns:
            (X, y, sample_weights): íŠ¹ì§• ë°ì´í„°í”„ë ˆì„, ë¼ë²¨ ì‹œë¦¬ì¦ˆ, ìƒ˜í”Œ ê°€ì¤‘ì¹˜ ë°°ì—´
        """
        with sqlite3.connect(self.db_path) as conn:
            df = pd.read_sql_query("""
                SELECT
                    rsi, macd, macd_signal, bb_position, volume_ratio,
                    price_change_5m, price_change_15m, ema_9, ema_21, atr,
                    COALESCE(hour_of_day, 12) as hour_of_day,
                    COALESCE(day_of_week, 0) as day_of_week,
                    COALESCE(rsi_change, 0) as rsi_change,
                    COALESCE(volume_trend, 0) as volume_trend,
                    COALESCE(rsi_prev_5m, rsi) as rsi_prev_5m,
                    COALESCE(bb_position_prev_5m, bb_position) as bb_position_prev_5m,
                    COALESCE(profit_class,
                        CASE
                            WHEN profit_rate < -0.005 THEN 0
                            WHEN profit_rate > 0.005 THEN 2
                            ELSE 1
                        END
                    ) as profit_class,
                    timestamp
                FROM trades
                WHERE status = 'closed' AND is_profitable IS NOT NULL
                ORDER BY timestamp DESC
                LIMIT ?
            """, conn, params=(limit,))

            # ìµœì‹ ìˆœ(DESC)ìœ¼ë¡œ ê°€ì ¸ì™”ìœ¼ë¯€ë¡œ ë‹¤ì‹œ ì‹œê°„ìˆœ(ASC)ìœ¼ë¡œ ì •ë ¬
            df = df.iloc[::-1].reset_index(drop=True)

        if len(df) < min_samples:
            logger.warning(f"âš ï¸ Insufficient data: {len(df)}/{min_samples}")
            return None

        # ğŸ†• ì‹œê°„ ê¸°ë°˜ ê°€ì¤‘ì¹˜ ê³„ì‚° (Exponential Time Decay)
        from datetime import datetime as dt

        # timestampë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
        df['timestamp'] = pd.to_datetime(df['timestamp'])

        # ê°€ì¥ ìµœê·¼ ê±°ë˜ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ ì¼ìˆ˜ ì°¨ì´ ê³„ì‚°
        latest_time = df['timestamp'].max()
        df['days_old'] = (latest_time - df['timestamp']).dt.total_seconds() / 86400  # ì´ˆ -> ì¼

        # Exponential decay ê°€ì¤‘ì¹˜ ê³„ì‚°
        # weight = exp(-decay_rate * days_old)
        # decay_rate = 0.02: ì•½ 35ì¼ë§ˆë‹¤ ê°€ì¤‘ì¹˜ ì ˆë°˜ìœ¼ë¡œ ê°ì†Œ
        decay_rate = 0.02
        sample_weights = np.exp(-decay_rate * df['days_old'].values)

        # ìµœì†Œ ê°€ì¤‘ì¹˜ 0.1 ë³´ì¥ (ì™„ì „íˆ ë¬´ì‹œë˜ì§€ ì•Šë„ë¡)
        sample_weights = np.maximum(sample_weights, 0.1)

        logger.info(f"ğŸ“Š Learning Data Loaded: {len(df)} samples (16 features, 3-class label)")
        logger.info(f"âš–ï¸  Sample Weights: min={sample_weights.min():.3f}, max={sample_weights.max():.3f}, "
                   f"mean={sample_weights.mean():.3f}")
        logger.info(f"ğŸ“… Data Age Range: {df['days_old'].min():.1f} ~ {df['days_old'].max():.1f} days")

        # timestampì™€ days_old ì»¬ëŸ¼ ì œê±°
        X = df.drop(['profit_class', 'timestamp', 'days_old'], axis=1)
        y = df['profit_class']

        return X, y, sample_weights
    
    def get_statistics(self) -> Dict:
        """í˜„ì¬ ë§¤ë§¤ í†µê³„ ë°˜í™˜"""
        with sqlite3.connect(self.db_path) as conn:
            stats = conn.execute("""
                SELECT 
                    COUNT(*) as total_trades,
                    SUM(CASE WHEN is_profitable = 1 THEN 1 ELSE 0 END) as wins,
                    AVG(profit_rate) as avg_profit,
                    MAX(profit_rate) as max_profit,
                    MIN(profit_rate) as max_loss
                FROM trades
                WHERE status = 'closed'
            """).fetchone()
        
        total, wins, avg_profit, max_profit, max_loss = stats
        win_rate = (wins / total * 100) if total > 0 else 0
        
        return {
            "total_trades": total or 0,
            "win_rate": win_rate,
            "avg_profit_pct": (avg_profit or 0) * 100,
            "max_profit_pct": (max_profit or 0) * 100,
            "max_loss_pct": (max_loss or 0) * 100
        }
    
    def get_open_positions(self) -> list:
        """
        ì—´ë¦° í¬ì§€ì…˜(open) ì¡°íšŒ - ì¬ì‹œì‘ ì‹œ ë³´ìœ  ì‹œê°„ ìœ ì§€ìš©
        
        Returns:
            list: [{"id": trade_id, "ticker": ticker, "entry_price": price, "entry_time": timestamp}, ...]
        """
        with sqlite3.connect(self.db_path) as conn:
            rows = conn.execute("""
                SELECT id, ticker, entry_price, timestamp
                FROM trades
                WHERE status = 'open'
            """).fetchall()
        
        positions = []
        for row in rows:
            positions.append({
                "id": row[0],
                "ticker": row[1],
                "entry_price": row[2],
                "entry_time": row[3]  # ISO format string
            })
        
        logger.info(f"ğŸ“‚ Found {len(positions)} open positions in DB")
        return positions


class ModelLearner:
    """
    XGBoost ëª¨ë¸ í•™ìŠµ ë° ê´€ë¦¬
    
    Features:
    - ì´ˆê¸° í•™ìŠµ (Cold Start)
    - ì ì§„ì  ì¬í•™ìŠµ (Incremental Update)
    - ëª¨ë¸ ì˜êµ¬ ì €ì¥/ë¡œë“œ
    - ì˜ˆì¸¡ ë° í™•ì‹ ë„ ì œê³µ
    """
    
    def __init__(self, model_path: str = None):
        # ê¸°ë³¸ ê²½ë¡œëŠ” í”„ë¡œì íŠ¸ ë£¨íŠ¸ì˜ models/xgb_model.pkl
        if model_path is None:
            model_path = str(PROJECT_ROOT / "models" / "xgb_model.pkl")
        self.model_path = model_path
        self.model: Optional[xgb.XGBClassifier] = None
        self.scaler: Optional[object] = None  # ğŸ†• StandardScaler ì €ì¥
        self.pca: Optional[object] = None     # ğŸ†• PCA ê°ì²´ ì €ì¥
        self.use_pca = True                   # PCA ì‚¬ìš© ì—¬ë¶€
        self.pca_components = 0.95            # 95% ë¶„ì‚° ë³´ì¡´
        self.metrics = {
            "accuracy": 0.0,
            "last_trained": None,
            "total_samples": 0
        }

        # ë””ë ‰í† ë¦¬ ìƒì„±
        Path(model_path).parent.mkdir(parents=True, exist_ok=True)

        # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹œë„
        self.load_model()
        logger.info("âœ… ModelLearner initialized")
    
    def train_initial_model(self, X: pd.DataFrame, y: pd.Series, sample_weights: np.ndarray = None):
        """
        ì´ˆê¸° ëª¨ë¸ í•™ìŠµ (Cold Start)
        ğŸ†• ì‹œê°„ ê°€ì¤‘ì¹˜ë¥¼ ë°˜ì˜í•œ í•™ìŠµ

        Args:
            X: íŠ¹ì§• ë°ì´í„°
            y: ë¼ë²¨
            sample_weights: ìƒ˜í”Œë³„ ê°€ì¤‘ì¹˜ (ìµœê·¼ ë°ì´í„° ìš°ì„ )
        """
        logger.info("ğŸ“ Starting Initial Model Training...")

        # ğŸ›¡ï¸ NaN ì²˜ë¦¬ (í•™ìŠµ ì „ ê²°ì¸¡ì¹˜ ì¤‘ì•™ê°’ìœ¼ë¡œ ëŒ€ì²´ - Outlier Detection ì „ì— ìˆ˜í–‰)
        # 0ìœ¼ë¡œ ëŒ€ì²´í•˜ë©´ ì •ë³´ ì™œê³¡ ê°€ëŠ¥, ì¤‘ì•™ê°’ì´ ë” ì•ˆì •ì 
        X = X.fillna(X.median())

        # ğŸ”¥ Outlier Detection (ì´ìƒê°’ ì œê±°)
        original_samples = len(X)
        if original_samples >= 30:  # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ ì ìš©
            from sklearn.ensemble import IsolationForest

            outlier_detector = IsolationForest(
                contamination=0.1,  # ë°ì´í„°ì˜ 10%ë¥¼ ì´ìƒê°’ìœ¼ë¡œ ê°„ì£¼
                random_state=42,
                n_jobs=-1
            )

            # ì´ìƒê°’ ê°ì§€ (-1: ì´ìƒê°’, 1: ì •ìƒê°’)
            is_inlier = outlier_detector.fit_predict(X)

            # ì •ìƒ ë°ì´í„°ë§Œ í•„í„°ë§
            X_clean = X[is_inlier == 1]
            y_clean = y[is_inlier == 1]

            # ğŸ†• ê°€ì¤‘ì¹˜ë„ í•¨ê»˜ í•„í„°ë§
            if sample_weights is not None:
                sample_weights_clean = sample_weights[is_inlier == 1]
            else:
                sample_weights_clean = None

            outliers_removed = original_samples - len(X_clean)
            logger.info(f"ğŸ§¹ Outlier Detection:")
            logger.info(f"   Total Samples: {original_samples}")
            logger.info(f"   Outliers Removed: {outliers_removed} ({outliers_removed/original_samples*100:.1f}%)")
            logger.info(f"   Clean Samples: {len(X_clean)}")

            X = X_clean
            y = y_clean
            sample_weights = sample_weights_clean
        else:
            logger.info(f"âš ï¸ Skipping outlier detection (need 30+ samples, got {original_samples})")
        
        # ğŸ”§ í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸ ë° ë¦¬ë§¤í•‘
        unique_classes = sorted(y.unique())
        logger.info(f"ğŸ“Š Class distribution: {dict(y.value_counts())}")
        
        # í´ë˜ìŠ¤ê°€ 3ê°œ ë¯¸ë§Œì´ë©´ ì¡´ì¬í•˜ëŠ” í´ë˜ìŠ¤ë§Œìœ¼ë¡œ í•™ìŠµ
        num_classes = len(unique_classes)
        if num_classes < 3:
            logger.warning(f"âš ï¸ Only {num_classes} classes present. Remapping to 0-{num_classes-1}")
            # í´ë˜ìŠ¤ ë¦¬ë§¤í•‘ (0, 2 â†’ 0, 1)
            class_map = {c: i for i, c in enumerate(unique_classes)}
            y = y.map(class_map)

        # Train-Test Split (í´ë˜ìŠ¤ê°€ ì¶©ë¶„í•˜ë©´ stratify ì‚¬ìš©)
        # ğŸ†• sample_weightsë„ í•¨ê»˜ ë¶„í• 
        try:
            # ê° í´ë˜ìŠ¤ë³„ ìµœì†Œ 2ê°œ ì´ìƒ ìˆì–´ì•¼ stratify ê°€ëŠ¥
            can_stratify = all(y.value_counts() >= 2)
            if can_stratify:
                if sample_weights is not None:
                    X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(
                        X, y, sample_weights, test_size=0.2, random_state=42, stratify=y
                    )
                else:
                    X_train, X_test, y_train, y_test = train_test_split(
                        X, y, test_size=0.2, random_state=42, stratify=y
                    )
                    weights_train, weights_test = None, None
            else:
                logger.warning("âš ï¸ Not enough samples per class for stratify. Using random split.")
                if sample_weights is not None:
                    X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(
                        X, y, sample_weights, test_size=0.2, random_state=42
                    )
                else:
                    X_train, X_test, y_train, y_test = train_test_split(
                        X, y, test_size=0.2, random_state=42
                    )
                    weights_train, weights_test = None, None
        except Exception as e:
            logger.warning(f"âš ï¸ Stratify failed: {e}. Using random split.")
            if sample_weights is not None:
                X_train, X_test, y_train, y_test, weights_train, weights_test = train_test_split(
                    X, y, sample_weights, test_size=0.2, random_state=42
                )
            else:
                X_train, X_test, y_train, y_test = train_test_split(
                    X, y, test_size=0.2, random_state=42
                )
                weights_train, weights_test = None, None
        
        # ğŸ”§ Feature Normalization (StandardScaler)
        from sklearn.preprocessing import StandardScaler
        
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)  # í•™ìŠµ ë°ì´í„°ë¡œ fit + transform
        X_test_scaled = self.scaler.transform(X_test)        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” transformë§Œ
        
        
        logger.info("ğŸ”§ Feature Normalization Applied (StandardScaler)")
        
        # ğŸ†• PCA Dimensionality Reduction
        # ğŸ†• PCA Dimensionality Reduction
        # ğŸ”¥ ë°ì´í„°ê°€ ì¶©ë¶„í•  ë•Œë§Œ PCA ì ìš© (100ê°œ ì´ìƒ)
        if self.use_pca and len(X_train) >= 100:
            from sklearn.decomposition import PCA
            self.pca = PCA(n_components=self.pca_components)
            X_train_final = self.pca.fit_transform(X_train_scaled)
            X_test_final = self.pca.transform(X_test_scaled)
            
            n_features_ = self.pca.n_components_
            explained_var_ = sum(self.pca.explained_variance_ratio_)
            logger.info(f"ğŸ§¬ PCA Applied: {X_train.shape[1]} -> {n_features_} features (Var={explained_var_:.1%})")
        else:
            if self.use_pca:
                logger.info(f"âš ï¸ Not enough data for PCA ({len(X_train)} samples). Skipping PCA.")
            self.pca = None
            X_train_final = X_train_scaled
            X_test_final = X_test_scaled
        
        # ğŸ†• XGBoost Multi-Class Model (ë™ì  í´ë˜ìŠ¤ ìˆ˜)
        self.model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=3 if len(X_train) < 100 else 5,  # ë°ì´í„° ì ì„ ë• ì–•ì€ íŠ¸ë¦¬
            learning_rate=0.1,
            objective='multi:softprob' if num_classes > 2 else 'binary:logistic',
            eval_metric='mlogloss' if num_classes > 2 else 'logloss',
            num_class=num_classes if num_classes > 2 else None,
            n_jobs=-1,  # M3 ìµœì í™”: ëª¨ë“  ì½”ì–´ ì‚¬ìš©
            random_state=42,
            tree_method='hist'  # ë¹ ë¥¸ í•™ìŠµ
        )
        
        # í•™ìŠµ ìˆ˜í–‰ (ì •ê·œí™”ëœ ë°ì´í„° + ì‹œê°„ ê°€ì¤‘ì¹˜ ì‚¬ìš©)
        fit_params = {
            'eval_set': [(X_test_final, y_test)],
            'verbose': False
        }

        # ğŸ†• ì‹œê°„ ê°€ì¤‘ì¹˜ ì¶”ê°€
        if weights_train is not None:
            fit_params['sample_weight'] = weights_train
            logger.info(f"âš–ï¸  Using time-weighted samples (recent data prioritized)")

        self.model.fit(X_train_final, y_train, **fit_params)
        
        # í‰ê°€
        y_pred = self.model.predict(X_test_final)
        accuracy = accuracy_score(y_test, y_pred)
        
        # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        self.metrics = {
            "accuracy": accuracy,
            "last_trained": datetime.now().isoformat(),
            "total_samples": len(X)
        }
        
        # ëª¨ë¸ ì €ì¥
        self.save_model()
        
        logger.info(f"âœ… Initial Training Complete - Accuracy: {accuracy:.2%}")
        logger.info(f"ğŸ“Š Classification Report:\n{classification_report(y_test, y_pred)}")
    
    def retrain_model(self, X: pd.DataFrame, y: pd.Series, sample_weights: np.ndarray = None):
        """
        ëª¨ë¸ ì¬í•™ìŠµ (Incremental Update)
        ğŸ†• ì‹œê°„ ê°€ì¤‘ì¹˜ë¥¼ ë°˜ì˜í•œ ì¬í•™ìŠµ

        ìƒˆë¡œìš´ ë§¤ë§¤ ë°ì´í„°ë¥¼ í¬í•¨í•˜ì—¬ ëª¨ë¸ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        XGBoostëŠ” ê¸°ë³¸ì ìœ¼ë¡œ incremental learningì„ ì™„ë²½ ì§€ì›í•˜ì§€ ì•Šì§€ë§Œ,
        ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ êµ¬í˜„í•©ë‹ˆë‹¤.
        """
        logger.info("ğŸ”„ Retraining Model with New Data...")

        # ì „ì²´ ë°ì´í„°ë¡œ ì¬í•™ìŠµ (ì‹œê°„ ê°€ì¤‘ì¹˜ í¬í•¨)
        self.train_initial_model(X, y, sample_weights)

        logger.info(f"âœ… Retraining Complete - New Accuracy: {self.metrics['accuracy']:.2%}")
    
    def predict(self, features: pd.DataFrame) -> Tuple[int, float]:
        """
        ì˜ˆì¸¡ ìˆ˜í–‰ (3-class ë¶„ë¥˜)
        
        Returns:
            (prediction, confidence): 
                - prediction: 0 (í°ì†ì‹¤), 1 (ì†Œí­), 2 (ì¢‹ì€ìˆ˜ìµ)
                - confidence: "ì¢‹ì€ ìˆ˜ìµ" í™•ë¥  (class 2ì˜ í™•ë¥ )
        """
        if self.model is None:
            logger.warning("âš ï¸ Model not trained yet!")
            return 0, 0.0
        
        # ğŸ†• 16ê°œ íŠ¹ì§• í™•ì¸ ë° ëˆ„ë½ëœ íŠ¹ì§• ì±„ìš°ê¸°
        expected_features = [
            'rsi', 'macd', 'macd_signal', 'bb_position', 'volume_ratio',
            'price_change_5m', 'price_change_15m', 'ema_9', 'ema_21', 'atr',
            'hour_of_day', 'day_of_week', 'rsi_change', 'volume_trend',
            'rsi_prev_5m', 'bb_position_prev_5m'
        ]
        
        # ëˆ„ë½ëœ íŠ¹ì§• ê¸°ë³¸ê°’ ì±„ìš°ê¸°
        for feat in expected_features:
            if feat not in features.columns:
                if feat == 'hour_of_day':
                    features[feat] = 12  # ì •ì˜¤
                elif feat == 'day_of_week':
                    features[feat] = 0   # ì›”ìš”ì¼
                elif feat == 'rsi_prev_5m':
                    features[feat] = features.get('rsi', 50)
                elif feat == 'bb_position_prev_5m':
                    features[feat] = features.get('bb_position', 0.5)
                else:
                    features[feat] = 0
        
        # íŠ¹ì§• ìˆœì„œ ë§ì¶”ê¸°
        features = features[expected_features]
        
        # ğŸ›¡ï¸ NaN ì²˜ë¦¬ (PCA ì˜¤ë¥˜ ë°©ì§€)
        features = features.fillna(0)
        
        # ğŸ”§ Feature Normalization ì ìš© (í•™ìŠµ ì‹œì™€ ë™ì¼í•œ Scaler ì‚¬ìš©)
        if self.scaler is not None:
            features_scaled = self.scaler.transform(features)
        else:
            # Scaler ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš© (í•˜ìœ„ í˜¸í™˜)
            features_scaled = features
        
        # ğŸ†• PCA Dimensionality Reduction ì ìš©
        if self.pca is not None:
            features_final = self.pca.transform(features_scaled)
        else:
            features_final = features_scaled
        
        # ì˜ˆì¸¡
        prediction = self.model.predict(features_final)[0]
        probabilities = self.model.predict_proba(features_final)[0]
        
        # ğŸ†• Class 2 (ì¢‹ì€ ìˆ˜ìµ) í™•ë¥ ì„ confidenceë¡œ ì‚¬ìš©
        # probabilities: [P(loss), P(neutral), P(profit)]
        confidence = probabilities[2] if len(probabilities) == 3 else probabilities[1]
        
        return int(prediction), float(confidence)
    
    def save_model(self):
        """ëª¨ë¸ì„ ë””ìŠ¤í¬ì— ì €ì¥"""
        if self.model is not None:
            joblib.dump({
                "model": self.model,
                "scaler": self.scaler,  # ğŸ†• Scalerë„ í•¨ê»˜ ì €ì¥
                "pca": self.pca,        # ğŸ†• PCA ì €ì¥
                "metrics": self.metrics
            }, self.model_path)
            logger.info(f"ğŸ’¾ Model saved to {self.model_path}")
    
    def load_model(self):
        """ì €ì¥ëœ ëª¨ë¸ ë¡œë“œ"""
        if Path(self.model_path).exists():
            data = joblib.load(self.model_path)
            self.model = data["model"]
            self.scaler = data.get("scaler", None)  # ğŸ†• Scaler ë¡œë“œ (í•˜ìœ„ í˜¸í™˜)
            self.pca = data.get("pca", None)        # ğŸ†• PCA ë¡œë“œ (í•˜ìœ„ í˜¸í™˜)
            self.metrics = data["metrics"]
            logger.info(f"ğŸ“‚ Model loaded from {self.model_path}")
            logger.info(f"   Accuracy: {self.metrics['accuracy']:.2%}")
            if self.scaler:
                logger.info("   âœ… Scaler loaded (Feature Normalization enabled)")
        else:
            logger.info("â„¹ï¸  No existing model found. Will train from scratch.")


class FeatureEngineer:
    """
    ê¸°ìˆ ì  ì§€í‘œ ê¸°ë°˜ íŠ¹ì§• ì¶”ì¶œ
    
    ê³¼ê±° ë°ì´í„°ë¥¼ ë°›ì•„ Machine Learningì— ì‚¬ìš©í•  íŠ¹ì§•(Features)ì„ ìƒì„±í•©ë‹ˆë‹¤.
    """
    
    @staticmethod
    def extract_features(df: pd.DataFrame) -> Dict:
        """
        OHLCV ë°ì´í„°ë¡œë¶€í„° ê¸°ìˆ ì  ì§€í‘œ ì¶”ì¶œ (í™•ì¥ ë²„ì „)
        
        Args:
            df: OHLCV ì»¬ëŸ¼ì„ ê°€ì§„ DataFrame (close, high, low, volume)
        
        Returns:
            features: ì¶”ì¶œëœ íŠ¹ì§• ë”•ì…”ë„ˆë¦¬ (16ê°œ íŠ¹ì§•)
        """
        from datetime import datetime
        
        # ìµœì†Œ ë°ì´í„° ê²€ì¦
        if len(df) < 30:
            logger.warning("âš ï¸ Insufficient data for feature extraction")
            return {}
        
        close = df['close']
        high = df['high']
        low = df['low']
        volume = df['volume']
        
        # 1. RSI (Relative Strength Index)
        rsi_series = RSIIndicator(close, window=14).rsi()
        rsi = rsi_series.iloc[-1]
        
        # 2. MACD
        macd_indicator = MACD(close)
        macd = macd_indicator.macd().iloc[-1]
        macd_signal = macd_indicator.macd_signal().iloc[-1]
        
        # 3. Bollinger Bands
        bb = BollingerBands(close, window=20, window_dev=2)
        bb_high = bb.bollinger_hband()
        bb_low = bb.bollinger_lband()
        current_price = close.iloc[-1]
        # BB ë‚´ ìƒëŒ€ ìœ„ì¹˜ (0: í•˜ë‹¨, 0.5: ì¤‘ê°„, 1: ìƒë‹¨)
        bb_h = bb_high.iloc[-1]
        bb_l = bb_low.iloc[-1]
        bb_position = (current_price - bb_l) / (bb_h - bb_l) if bb_h != bb_l else 0.5
        
        # 4. Volume Ratio
        volume_ma = volume.rolling(window=20).mean().iloc[-1]
        volume_ratio = volume.iloc[-1] / volume_ma if volume_ma > 0 else 1.0
        
        # 5. Price Change
        price_change_5m = (close.iloc[-1] - close.iloc[-5]) / close.iloc[-5] if len(close) >= 5 else 0
        price_change_15m = (close.iloc[-1] - close.iloc[-15]) / close.iloc[-15] if len(close) >= 15 else 0
        
        # 6. EMA (Exponential Moving Average)
        ema_9 = EMAIndicator(close, window=9).ema_indicator().iloc[-1]
        ema_21 = EMAIndicator(close, window=21).ema_indicator().iloc[-1]
        
        # 7. ATR (Average True Range) - ë³€ë™ì„± ì¸¡ì •
        atr = AverageTrueRange(high, low, close, window=14).average_true_range().iloc[-1]
        
        # ============ ğŸ†• NEW FEATURES ============
        
        # 8. Time Features (ì‹œê°„ íŠ¹ì§•)
        now = datetime.now()
        hour_of_day = now.hour        # 0-23
        day_of_week = now.weekday()   # 0-6 (ì›”-ì¼)
        
        # 9. Momentum Features (ëª¨ë©˜í…€ íŠ¹ì§•)
        # RSI ë³€í™”ëŸ‰ (5ë¶„ ì „ ëŒ€ë¹„)
        rsi_prev_5m = rsi_series.iloc[-5] if len(rsi_series) >= 5 else rsi
        rsi_change = rsi - rsi_prev_5m
        
        # ê±°ë˜ëŸ‰ ì¶”ì„¸ (ìµœê·¼ 5ê°œ vs ì´ì „ 5ê°œ)
        if len(volume) >= 10:
            recent_vol = volume.iloc[-5:].mean()
            prev_vol = volume.iloc[-10:-5].mean()
            volume_trend = (recent_vol - prev_vol) / prev_vol if prev_vol > 0 else 0
        else:
            volume_trend = 0
        
        # 10. Sequence Features (ì‹œê³„ì—´ íŠ¹ì§•)
        # 5ë¶„ ì „ BB ìœ„ì¹˜
        if len(bb_high) >= 5 and len(bb_low) >= 5:
            bb_h_5m = bb_high.iloc[-5]
            bb_l_5m = bb_low.iloc[-5]
            price_5m = close.iloc[-5]
            bb_position_prev_5m = (price_5m - bb_l_5m) / (bb_h_5m - bb_l_5m) if bb_h_5m != bb_l_5m else 0.5
        else:
            bb_position_prev_5m = bb_position
        
        features = {
            # ê¸°ì¡´ íŠ¹ì§• (10ê°œ)
            'rsi': rsi,
            'macd': macd,
            'macd_signal': macd_signal,
            'bb_position': bb_position,
            'volume_ratio': volume_ratio,
            'price_change_5m': price_change_5m,
            'price_change_15m': price_change_15m,
            'ema_9': ema_9,
            'ema_21': ema_21,
            'atr': atr,
            # ğŸ†• ì‹œê°„ íŠ¹ì§• (2ê°œ)
            'hour_of_day': hour_of_day,
            'day_of_week': day_of_week,
            # ğŸ†• ëª¨ë©˜í…€ íŠ¹ì§• (2ê°œ)
            'rsi_change': rsi_change,
            'volume_trend': volume_trend,
            # ğŸ†• ì‹œê³„ì—´ íŠ¹ì§• (2ê°œ)
            'rsi_prev_5m': rsi_prev_5m,
            'bb_position_prev_5m': bb_position_prev_5m
        }
        
        return features
    
    @staticmethod
    def features_to_dataframe(features: Dict) -> pd.DataFrame:
        """íŠ¹ì§• ë”•ì…”ë„ˆë¦¬ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜ (ëª¨ë¸ ì…ë ¥ìš©)"""
        return pd.DataFrame([features])


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    print("=" * 60)
    print("Data & Model Manager Test")
    print("=" * 60)
    
    # TradeMemory í…ŒìŠ¤íŠ¸
    memory = TradeMemory()
    print("\nâœ… TradeMemory created")
    
    # ModelLearner í…ŒìŠ¤íŠ¸
    learner = ModelLearner()
    print("âœ… ModelLearner created")
    
    # í†µê³„ í™•ì¸
    stats = memory.get_statistics()
    print(f"\nğŸ“Š Current Statistics:")
    print(f"   Total Trades: {stats['total_trades']}")
    print(f"   Win Rate: {stats['win_rate']:.2f}%")
    
    print("\n" + "=" * 60)
    print("âœ… All tests passed!")
    print("=" * 60)
